{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4a - Introduction to LLM Evaluations With TruLens\n",
    "In this exercise you'll learn how to assess the performance of a RAG application using the Trulens framework. We'll introduce the key concepts involved in testing an LLM-powered application, run a basic answer relevance test and view the results in a test dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the RAG application is imported and configured. In this case, it is a [langchain](https://www.langchain.com/) application which wraps an instance of [Azure Information Assistant](https://github.com/microsoft/PubSec-Info-Assistant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config_file_name: str = \"config-4a.json\"\n",
    "current_dir_path: Path = Path(\".\")\n",
    "full_path = current_dir_path / config_file_name\n",
    "\n",
    "with full_path.open() as f:\n",
    "    config = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(str(current_dir_path.resolve().parent.parent))\n",
    "from llm_application.azure_information_assistant_accelerator.wrapper import RAG_from_scratch\n",
    "\n",
    "rag_chain = RAG_from_scratch(config_data=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App\n",
    "Now that the target application has been configured, we'll introduce the code that will be used to evaluate the application's performance.\n",
    "\n",
    "Our first step is to set up an App object, this will manage the tests that we conduct and track the results in a local database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kjr_llm.app import App\n",
    "\n",
    "app = App(app_name=\"exercise4a\", reset_database=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets\n",
    "Next we define the target of our tests, this is an abstraction which enables the framework to communicate with the application being tested. Preconfigured options exist for Langchain and LlamaIndex applications but in this case we opt to use a more flexible Custom target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kjr_llm.targets import CustomTarget\n",
    "\n",
    "target = CustomTarget(rag_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tests\n",
    "In TruLens, a test consists of two elements, prompts and feedback functions.\n",
    "\n",
    "##### Prompts\n",
    "\n",
    "In the context of evaluating LLM-powered applications, prompts refer to the input queries or instructions given to the model to generate responses or predictions. Prompts are critical for guiding the behavior of the model, as they frame the task or question the model is expected to address. By providing a range of carefully designed prompts, evaluators can test the modelâ€™s ability to handle different types of input, ensuring it responds accurately and appropriately.\n",
    "\n",
    "Effective prompt design is essential for thorough evaluation, as it reveals the model's strengths, limitations, and behavior across different scenarios.\n",
    "\n",
    "##### Prompt Sets\n",
    "Our evaluation framework provides the `Prompt` and `PromptSet` classes to faciliate loading and interacting with prompts. \n",
    "\n",
    "A `Prompt` is a single input provided to the target application. A `Prompt` consists of several fields, an input, an optional expected response from the application and an optional context. \n",
    "\n",
    "A `PromptSet` is a set of one or more related `Prompts`. A PromptSet is defined in JSON format. You can see an example of the JSON format in `./relevance-4a.json`. \n",
    "\n",
    "In the code snippet below, we import the PromptSet class and load our prompts from the file they are defined in, then iterate over the prompt set and print the individual prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kjr_llm.prompts import PromptSet\n",
    "\n",
    "prompts_path = current_dir_path / \"relevance-4a.json\"\n",
    "prompts = PromptSet.from_json_file(prompts_path)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "Metrics evaluate a target model's performance when responding to one or more prompts. A metric assesses the models performance in regard to a specific category, such as the presence of hate speech, or the groundedness of the response in the provided context. \n",
    "\n",
    "Most metrics are backed by an LLM, known as the provider model. This can (but doesn't have to be) the same model used by the application being evaluated. The metric is essentially a prompt which asks the provider model to score the target applications response against a rubric.\n",
    "\n",
    "In some cases, for example when attempting to detect the presence of personal information in a response, it is necessary to use a provider model than can be run locally, such as llama3, to ensure that personal information is not exposed to a proprietary model where it could be retained for training purposes.\n",
    "\n",
    "In the code below, we import and instantiate the Answer Relevance metric which evaluates the relevance of the LLM response to the input prompt. Note that the openai property is selected which denotes the provider model to be used by the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kjr_llm.metrics import (\n",
    "    AnswerRelevance\n",
    ")\n",
    "from trulens.core.schema import Select\n",
    "\n",
    "# Using custom TestSet\n",
    "# comment and uncomment the feedback you wish to evaluate\n",
    "metrics = [\n",
    "    AnswerRelevance().openai\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some prompts to feed into the target application and a metric to assess its performance, they can be combined to produce a test set. The `TestSet` object provides an `evaluate` method which will execute the contained tests against our target application.\n",
    "\n",
    "A default provider can be set when creating the test set which will be used by any metrics where a provider was not explicitly specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kjr_llm.tests import TestSet\n",
    "from kjr_llm.provider import OpenAIProvider\n",
    "\n",
    "gpt_35_turbo_provider = OpenAIProvider(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Define our test set\n",
    "custom_test = TestSet(prompts, metrics, name=\"Exercise4-openai\", \n",
    "                      default_provider=gpt_35_turbo_provider)\n",
    "\n",
    "# Evaluate our test set\n",
    "result = custom_test.evaluate(target, \"Exercise4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard\n",
    "Once the tests have been executed, the `App` object can be used to run a local test dashboard and peruse the results. The dashboard uses the `streamlit` library and will attempt to open automatically in your default browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
