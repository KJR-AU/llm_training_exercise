{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Groundedness and Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a set of relevance and groundedness tests using the provided documents.\n",
    "    - Review the documents and create a set of prompts which extract specific facts from those documents.\n",
    "    Open [JSON prompt file](relevance_and_groundedness_prompts.json) and add prompts (question) in input field.\n",
    "    - Create a set of expected results as ground truth for those prompts.\n",
    "    Open [JSON prompt file](relevance_and_groundedness_prompts.json) and add expected results in expected_output field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the tests against each target RAG system.\n",
    "    - There are 'openai' and 'llama3' provider available.\n",
    "    - in the below cell, change custom_test = TestSet(prompts, feedbacks, name=\"Exercise4\", default_provider=\"openai\") openai to llama3 if you wish to evaluate with llama3.\n",
    "\n",
    "RAG execution test files - and observe the results carefully and evaluate which RAG system performs better.\n",
    "1. [openai](grounded_relevance_openai.py)\n",
    "2. [llama3](grounded_relevance_llama3.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to [RAG application config file](./../../llm_application/azure_information_assistant_accelerator/config.json) and change folder to change the documents being retrieved.\n",
    "Note: \"All\" means all folders existing.\n",
    "\n",
    "Now re-run the tests and observe how the results differ.\n",
    "1. [openai](grounded_relevance_openai.py)\n",
    "2. [llama3](grounded_relevance_llama3.py)\n",
    "\n",
    "And examine the chain of thought responses for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('__file__'), '..', '..')))\n",
    "\n",
    "from llm_application.azure_information_assistant_accelerator.wrapper import rag_chain\n",
    "from kjr_llm.targets import CustomTarget\n",
    "from kjr_llm.app import App\n",
    "from kjr_llm.tests import TestSet\n",
    "from kjr_llm.targets import Target\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from kjr_llm.prompts import PromptSet\n",
    "from trulens_eval import Select\n",
    "from kjr_llm.metrics import (\n",
    "    Groundedness, \n",
    "    AnswerRelevance,\n",
    "    ContextRelevance,\n",
    "    GroundTruthAgreement\n",
    ")\n",
    "\n",
    "\n",
    "# Set up the test application\n",
    "app = App(app_name=\"RAG_Application\", reset_database=True)\n",
    "\n",
    "# Define the target of our tests\n",
    "target: Target = CustomTarget(rag_chain)\n",
    "\n",
    "# Load our custom inputs - change the name of this file if you have prepared another prompts.\n",
    "prompts = PromptSet.from_json_file(\"test_prompts.json\")\n",
    "\n",
    "# Import and instantiate feedback metrics\n",
    "query_path = Select.Record.app.query.args.query\n",
    "context_path = Select.Record.app.retrieve.rets[:]\n",
    "\n",
    "# Using custom TestSet\n",
    "# comment and uncomment the feedback you wish to evaluate\n",
    "feedbacks = [\n",
    "    Groundedness(context_path),\n",
    "    ContextRelevance(query_path, context_path),\n",
    "    AnswerRelevance(),\n",
    "    GroundTruthAgreement(prompts)\n",
    "]\n",
    "\n",
    "# Define our test set - switch comment to evaluate llama3.\n",
    "custom_test = TestSet(prompts, feedbacks, name=\"Exercise4-openai\", default_provider=\"openai\")\n",
    "# custom_test = TestSet(prompts, feedbacks, name=\"Exercise4-llama3\", default_provider=\"llama3\")\n",
    "\n",
    "# Evaluate our test set\n",
    "result = custom_test.evaluate(target)\n",
    "\n",
    "# Run the test dashboard to evaluate results\n",
    "app.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to stop the dashboard\n",
    "app.stop_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
