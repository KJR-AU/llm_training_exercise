{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Exercise - Topic Detection\n",
    "This exercise demonstrates how to use a language model (LLM) to classify text into predefined topics and determine how well it is performing. We'll explore topic detection using a simple dataset and employ a confusion matrix to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "We begin by loading the required environment variables using the `dotenv` package. This allows us to securely load API keys or other configuration settings from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preparing the Dataset\n",
    "The first step when evaluating the performance of a model is to ensure sufficient ground truth exists. Ground truth refers to the actual, correct labels or values in a dataset that serve as the standard for evaluating the model's performance. It is crucial because it provides a baseline for comparison, allowing us to assess how accurately a model's predictions align with real-world outcomes. Without ground truth, it would be impossible to measure the model's correctness or identify areas for improvement.\n",
    "\n",
    "Ground truth data is often produced through manual labelling tasks where human experts annotate data based on established criteria. While this process generally produces high-quality, accurate labels it is both time-consuming and expensive, particularly when working with large datasets, as significant effort and attention is required. \n",
    "\n",
    "For the purpose of this exercise, we will be assessing the performance of a simple, LLM-powered classification model. To generate the ground truth required to assess its performance, you will perform some manual labelling in the cell below. \n",
    "\n",
    "A dataset of fifteen sentences has been defined which we will attempt to classify into one of four categories based on the primary topic of discussion. For each sentence, fill in the empty string with the label you feel best represents the topic being discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Technology','Sports','Language','Food']\n",
    "\n",
    "# From the given labels, add a ground truth topic label to each statement below. \n",
    "# The first statement has been labelled for you.\n",
    "dataset = [ # [Topic, Text to classify]\n",
    "    ['Technology', \"Technology shapes our lives, from smartphones to algorithms. It drives innovation and connects us in ways we couldn't imagine\"],\n",
    "    ['', \"AI ethics is a critical consideration in developing responsible algorithms.\"],\n",
    "    ['', \"Language is the expression of ideas through speech-sounds and words.\"], \n",
    "    ['', \"Words are combined into sentences, answering to ideas into thoughts.\"], \n",
    "    ['', \"Content moderation on social media platforms detect and filter out inappropriate language and harmful content to maintain a respectful and safe online environment\"],\n",
    "    ['', \"Speech-to-text software has become crucial for accessibility, allowing users to transcribe spoken language into written text efficiently\"],\n",
    "    ['', \"Language is a dynamic system of communication that evolves over time, reflecting cultural, social, and historical changes in society.\"],\n",
    "    ['', \"Golden State Warriors seek a second star alongside Stephen Curry.\"], \n",
    "    ['', \"San Francisco 49ers maintain a successful offensive strategy.\"], \n",
    "    ['', \"In the case of food establishments, like most sports, the first line of defense are the players in the game, which are the industry that produces the products.\"],\n",
    "    ['', \"After a thrilling soccer match, fans celebrate with stadium hot dogs and cold beverages.\"],\n",
    "    ['', \"Athletes know that proper nutrition is as crucial as their training regimen.\"],\n",
    "    ['', \"In the culinary Olympics, the gold medal goes to the chef who masters flavor balance.\"],\n",
    "    ['', \"Basketball players fuel up with protein-packed meals before hitting the court.\"],\n",
    "    ['', \"The marathon of cooking competitions leaves chefs both exhausted and exhilarated.\"],\n",
    "]\n",
    "\n",
    "for topic, text in dataset:\n",
    "    if not topic:\n",
    "        topic_sentence: str = ', '.join(labels)\n",
    "        raise ValueError(f'Ensure that each sentence is labelled with one of the following topics: {topic_sentence}')\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset, columns=['Topic', 'Text'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Running the Topic Detection Task\n",
    "Next we define the classification model that we'll be evaluating. We're using Langchain, a widely-used framework for building LLM-powered applications, with Open AI's GPT 3.5 Turbo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Extract the desired information from the following passage.\n",
    "\n",
    "    Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "    Passage:\n",
    "    {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    Topic: str = Field(description=\"Choose at most one topic from this list: \" + ''.join(labels) + \" that are related to the content\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\").with_structured_output(\n",
    "    Classification\n",
    ")\n",
    "\n",
    "tagging_chain = tagging_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling our Dataset\n",
    "Now that we've built our model, we can use it to predict labels for our dataset. These labels can then be compared against the ground truth labels we defined earlier to assess the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "expected_result = []\n",
    "for idx, topic, text in df.itertuples():\n",
    "    expected_result.append(topic)\n",
    "    result = tagging_chain.invoke({\"input\": text})\n",
    "    results.append(result.Topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Results\n",
    "Here, we display the ground truth and the predicted topic for each text entry. This will help us visually assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = list(zip(expected_result, results, df['Text']))\n",
    "df2 = pd.DataFrame(combined_data, columns=['ground truth', 'predicted','text'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Performance Evaluation\n",
    "We now evaluate the model's performance in greater detail using a confusion matrix.\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model. It compares the actual labels (ground truth) with the predicted labels from the model, providing a detailed breakdown of the model's performance across different classes.\n",
    "\n",
    "In the case of a non-binary (multiclass) classifier, a confusion matrix provides a detailed view of how well the model is performing across multiple classes, rather than just two. It is an extension of the binary confusion matrix to handle more than two possible labels. The matrix layout changes as follows:\n",
    "\n",
    "* The rows represent the actual (true) classes.\n",
    "* The columns represent the predicted classes.\n",
    "* Each cell in the matrix represents the number of instances where a specific true class was predicted as a specific predicted class.\n",
    "\n",
    "For a non-binary classifier with n classes, the confusion matrix will be an n√ón grid. \n",
    "\n",
    "A confusion matrix helps identify where a model makes errors, such as mistaking one class for another. This analysis is crucial for understanding the strengths and weaknesses of a classifier beyond simple accuracy, enabling more refined metrics like precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Sample true labels and predicted labels\n",
    "y_true = np.array(expected_result)\n",
    "y_pred = np.array(results)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "report = classification_report(y_true, y_pred, labels=labels, target_names=labels, zero_division=0)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Analysis Using TP, FP, TN, and FN\n",
    "We can dig deeper into the performance of the model by thoroughly evaluating performance on each class. We do this by turning performance on each class into a binary classification task. We then produce a distinct confusion matrix for each class and calculate metrics like accuracy, precision, recall and f1 scores. In order to understand these metrics, we must first discuss understand how binary classification tasks are assessed.\n",
    "\n",
    "* True Positives (TP): The model correctly predicted the positive class.\n",
    "* False Positives (FP): The model incorrectly predicted the positive class when it was actually negative.\n",
    "* True Negatives (TN): The model correctly predicted the negative class.\n",
    "* False Negatives (FN): The model incorrectly predicted the negative class when it was actually positive.\n",
    "\n",
    "These data points can be summarised by the following metrics.\n",
    "\n",
    "* Accuracy measures the overall correctness of a model by calculating the proportion of correctly predicted instances (both true positives and true negatives) out of all predictions. It is useful when classes are balanced but can be misleading if they are imbalanced.\n",
    "\n",
    "* Precision (or Positive Predictive Value) focuses on the quality of positive predictions. It calculates the proportion of correctly predicted positive instances (true positives) out of all instances predicted as positive (true positives + false positives). High precision means fewer false positives.\n",
    "\n",
    "* Recall (or Sensitivity) measures the model's ability to correctly identify all positive instances. It calculates the proportion of actual positives that were correctly predicted (true positives) out of all actual positive cases (true positives + false negatives). High recall means fewer false negatives.\n",
    "\n",
    "* F1 score is a metric that balances precision and recall into a single number, providing a more comprehensive view of a model's performance, particularly when there is an imbalance between the two as it penalizes models that perform well in precision but poorly in recall, or vice versa. It is defined as the harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "def calculate_score(TN, FP, FN, TP):\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "    return ACC, PPV, TPR\n",
    "\n",
    "mcm = multilabel_confusion_matrix(y_true, y_pred,labels=labels)\n",
    "count = 0\n",
    "calculation_result = []\n",
    "# Display the confusion matrix\n",
    "for cm_i in mcm:\n",
    "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_i, display_labels=['Not '+ labels[count], labels[count]])\n",
    "    disp2.plot()\n",
    "\n",
    "    tn, fp, fn, tp = cm_i.ravel()\n",
    "    acc, prec, rec = calculate_score(tn, fp, fn, tp)\n",
    "    calculation_result.append([labels[count],tn, fp, fn, tp, acc, prec, rec])\n",
    "    count = count + 1\n",
    "\n",
    "df2 = pd.DataFrame(calculation_result, columns=['Topic','TN', 'FP','FN', 'TP', 'Accuracy','Precision','Recall'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df2)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
