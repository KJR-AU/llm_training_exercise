{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Exercise - Topic Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - [Topic, Text to classify]\n",
    "labels = ['Technology','Sports','Language','Food']\n",
    "\n",
    "# From the given label, add a ground truth topic label to each statement below.\n",
    "dataset = [\n",
    "    ['',\"Technology shapes our lives, from smartphones to algorithms. It drives innovation and connects us in ways we couldn't imagine\"],\n",
    "    ['', \"AI ethics is a critical consideration in developing responsible algorithms.\"],\n",
    "    ['', \"Language is the expression of ideas through speech-sounds and words.\"], \n",
    "    ['', \"Words are combined into sentences, answering to ideas into thoughts.\"], \n",
    "    ['',\"Content moderation on social media platforms detect and filter out inappropriate language and harmful content to maintain a respectful and safe online environment\"],\n",
    "    ['',\"Speech-to-text software has become crucial for accessibility, allowing users to transcribe spoken language into written text efficiently\"],\n",
    "    ['',\"Language is a dynamic system of communication that evolves over time, reflecting cultural, social, and historical changes in society.\"],\n",
    "    ['', \"Golden State Warriors seek a second star alongside Stephen Curry.\"], \n",
    "    ['', \"San Francisco 49ers maintain a successful offensive strategy.\"], \n",
    "    ['','In the case of food establishments, like most sports, the first line of defense are the players in the game, which are the industry that produces the products.'],\n",
    "    ['','After a thrilling soccer match, fans celebrate with stadium hot dogs and cold beverages.'],\n",
    "    ['','Athletes know that proper nutrition is as crucial as their training regimen.'],\n",
    "    ['','In the culinary Olympics, the gold medal goes to the chef who masters flavor balance.'],\n",
    "    ['','Basketball players fuel up with protein-packed meals before hitting the court.'],\n",
    "    ['','The marathon of cooking competitions leaves chefs both exhausted and exhilarated.'],\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset, columns=['Topic', 'Text'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the topic detection test with a prompt to label each text with your one of your topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    Topic: str = Field(description=\"Choose at most one topic from this list: \" + ''.join(labels) + \" that are related to the content\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\").with_structured_output(\n",
    "    Classification\n",
    ")\n",
    "\n",
    "tagging_chain = tagging_prompt | llm\n",
    "\n",
    "sample = df\n",
    "results = []\n",
    "expected_result = []\n",
    "for i in sample.iterrows():\n",
    "    expected_result.append(i[1]['Topic'])\n",
    "    result = tagging_chain.invoke({\"input\": i[1]['Text']})\n",
    "    results.append(result.Topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here view the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = list(zip(expected_result, results, df['Text']))\n",
    "df2 = pd.DataFrame(combined_data, columns=['expected', 'actual','text'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the results of the confusion matrix and identify which kind of texts are not performing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Sample true labels and predicted labels\n",
    "y_true = np.array(expected_result)\n",
    "y_pred = np.array(results)\n",
    "\n",
    "print('expected value: ', y_true)\n",
    "print('actual value: ', y_pred)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "report = classification_report(y_true, y_pred, labels=labels, target_names=labels, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Code below this cell has more detail on TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "def calculate_score(TN, FP, FN, TP):\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "    return ACC, PPV, TPR\n",
    "\n",
    "mcm = multilabel_confusion_matrix(y_true, y_pred,labels=labels)\n",
    "count = 0\n",
    "calculation_result = []\n",
    "# Display the confusion matrix\n",
    "for cm_i in mcm:\n",
    "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_i, display_labels=['not '+ labels[count], labels[count]])\n",
    "    disp2.plot()\n",
    "\n",
    "    tn, fp, fn, tp = cm_i.ravel()\n",
    "    acc, prec, rec = calculate_score(tn, fp, fn, tp)\n",
    "    calculation_result.append([labels[count],tn, fp, fn, tp, acc, prec, rec])\n",
    "    count = count + 1\n",
    "\n",
    "df2 = pd.DataFrame(calculation_result, columns=['Topic','TN', 'FP','FN', 'TP', 'Accuracy','Precision','Recall'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore reasons for poor performance by modifying your prompt or modifying the texts to see what changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
